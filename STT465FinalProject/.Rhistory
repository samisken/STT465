# Check for NA values
apply(test_categorical_rp, 2, function(x) any(is.na(x)))
# Check for classes
apply(test_categorical_rp, 2, function(x) class(x))
newdf_train <- train_categorical_rp
newdf_test <- test_categorical_rp
nrow(newdf_test)
# Check for NA values
sum(apply(newdf_train, 2, function(x) any(is.na(x))))
sum(apply(newdf_test, 2, function(x) any(is.na(x))))
# Drop functional column
newdf_train=subset(newdf_train, select=-c(Functional))
newdf_test=subset(newdf_test, select=-c(Functional))
# Check for NA values
sum(apply(newdf_train, 2, function(x) any(is.na(x))))
sum(apply(newdf_test, 2, function(x) any(is.na(x))))
# Check for classes
apply(newdf_train, 2, function(x) class(x))
apply(newdf_test, 2, function(x) class(x))
summary(lm(log(SalePrice)~.,data=newdf_train))
# Create pre-processed dfs
train_pre_processed <- newdf_train
test_pre_processed <- newdf_test
# Convert train to numeric
must_convert<-sapply(imputed_train,is.factor)       # logical vector telling if a variable needs to be displayed as numeric
train_pre_processed_2 <-sapply(imputed_train[,must_convert],unclass)    # data.frame of all categorical variables now displayed as numeric
out<-cbind(imputed_train[,!must_convert],train_pre_processed_2)        # complete data.frame with all variables put together
train_numeric <- out
# Convert test to numeric
must_convert<-sapply(test_pre_processed,is.factor)       # logical vector telling if a variable needs to be displayed as numeric
test_pre_processed_2 <-sapply(test_pre_processed[,must_convert],unclass)    # data.frame of all categorical variables now displayed as numeric
out<-cbind(test_pre_processed[,!must_convert],test_pre_processed_2)        # complete data.frame with all variables put together
test_numeric <- out
# Check to ensure factors weren't reducted to 1 constant
length(unique(train$MSSubClass))
length(unique(train$Utilities))
length(unique(train$MSZoning))
length(unique(train$LotArea))
length(unique(train$LotConfig))
length(unique(train$LandSlope))
length(unique(train$Neighborhood))
length(unique(train$Condition1))
length(unique(train$Condition2))
length(unique(train$OverallQual))
length(unique(train$OverallCond))
length(unique(train$YearBuilt))
length(unique(train$YearRemodAdd))
length(unique(train$RoofStyle))
length(unique(train$RoofMatl))
length(unique(train$Exterior1st))
length(unique(train$MasVnrType))
length(unique(train$BsmtCond))
length(unique(train$BsmtExposure))
length(unique(train$BsmtFinSF1))
length(unique(train$BsmtFinSF2))
length(unique(train$BsmtUnfSF))
length(unique(train$HeatingQC))
length(unique(train$CentralAir))
length(unique(train$Electrical))
length(unique(train$X1stFlrSF))
length(unique(train$X2ndFlrSF))
length(unique(train$BsmtFullBath))
length(unique(train$FullBath))
length(unique(train$HalfBath))
length(unique(train$KitchenAbvGr))
length(unique(train$KitchenQual))
length(unique(train$Fireplaces))
length(unique(train$GarageType))
length(unique(train$GarageCars))
length(unique(train$GarageArea))
length(unique(train$GarageQual))
length(unique(train$WoodDeckSF))
length(unique(train$EnclosedPorch))
length(unique(train$ScreenPorch))
length(unique(train$PoolArea))
length(unique(train$SaleType))
length(unique(train$SaleCondition))
length(unique(train$Utilities))
# Train
corrplot(cor(train_numeric))
indexesToDrop <- findCorrelation(cor(train_numeric), cutoff = 0.8)
corrplot(cor(train_numeric[,-indexesToDrop]))
train_numeric_cl <- train_numeric[,-indexesToDrop]
# Test
corrplot(cor(test_numeric))
#indexesToDrop <- findCorrelation(cor(test_numeric), cutoff = 0.8)
corrplot(cor(test_numeric[,-indexesToDrop])) # drop the same indexes from the train and test df
test_numeric_cl <- test_numeric[,-indexesToDrop]
summary(lm_all_predictors_log)
train <- fwd_train_df
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(FNN)
library(leaps)
library(class)
library(glmnet)
library(boot)
library(tidyverse)
library(caret)
library(compare)
library(randomForest)
library(tree)
library(corrplot)
library(rpart)
library(rpart.plot)
library(imputeMissings)
test <- read.csv("Data/test.csv")
train <- read.csv("Data/train.csv")
head(train)
head(test)
hist(train$SalePrice,col="120")
hist(log(train$SalePrice),col="120")
# Create a function that, for a numeric column NA value, takes the mean
# Numeric Value
natomean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
# Run function on train and test
train <- replace(train, TRUE, lapply(train, natomean))
test <- replace(test, TRUE, lapply(test, natomean))
# Display head of train and test
head(train,10)
head(test,10)
# Create a function that, for a categorical NA value, takes the mode
# Categorical Data Cleaning
# Apply to train data
values2 <- compute(train)
imputed_train=impute(train,object = values2) #using median/mode values
head(imputed_train)
# Apply to test data
values2 <- compute(test)
imputed_test=impute(test,object = values2) #using median/mode values
head(imputed_test)
# Create dfs of train and test
train_categorical_rp <- imputed_train
test_categorical_rp <- imputed_test
# Drop ID and Alley (ID is NA, Alley did not seem very significant and was difficult to clean)
newdf_train <- subset(train_categorical_rp, select=-c(Id,Alley))
head(newdf_train,10)
newdf_test <- subset(test_categorical_rp, select=-c(Id,Alley))
head(newdf_test)
# Check for NA values
apply(train_categorical_rp, 2, function(x) any(is.na(x)))
# Check for classes
apply(train_categorical_rp, 2, function(x) class(x))
# Check for NA values
apply(test_categorical_rp, 2, function(x) any(is.na(x)))
# Check for classes
apply(test_categorical_rp, 2, function(x) class(x))
newdf_train <- train_categorical_rp
newdf_test <- test_categorical_rp
nrow(newdf_test)
# Check for NA values
sum(apply(newdf_train, 2, function(x) any(is.na(x))))
sum(apply(newdf_test, 2, function(x) any(is.na(x))))
# Drop functional column
newdf_train=subset(newdf_train, select=-c(Functional))
newdf_test=subset(newdf_test, select=-c(Functional))
# Check for NA values
sum(apply(newdf_train, 2, function(x) any(is.na(x))))
sum(apply(newdf_test, 2, function(x) any(is.na(x))))
# Check for classes
apply(newdf_train, 2, function(x) class(x))
apply(newdf_test, 2, function(x) class(x))
summary(lm(log(SalePrice)~.,data=newdf_train))
# Create pre-processed dfs
train_pre_processed <- newdf_train
test_pre_processed <- newdf_test
# Convert train to numeric
must_convert<-sapply(imputed_train,is.factor)       # logical vector telling if a variable needs to be displayed as numeric
train_pre_processed_2 <-sapply(imputed_train[,must_convert],unclass)    # data.frame of all categorical variables now displayed as numeric
out<-cbind(imputed_train[,!must_convert],train_pre_processed_2)        # complete data.frame with all variables put together
train_numeric <- out
# Convert test to numeric
must_convert<-sapply(test_pre_processed,is.factor)       # logical vector telling if a variable needs to be displayed as numeric
test_pre_processed_2 <-sapply(test_pre_processed[,must_convert],unclass)    # data.frame of all categorical variables now displayed as numeric
out<-cbind(test_pre_processed[,!must_convert],test_pre_processed_2)        # complete data.frame with all variables put together
test_numeric <- out
# Check to ensure factors weren't reducted to 1 constant
length(unique(train$MSSubClass))
length(unique(train$Utilities))
length(unique(train$MSZoning))
length(unique(train$LotArea))
length(unique(train$LotConfig))
length(unique(train$LandSlope))
length(unique(train$Neighborhood))
length(unique(train$Condition1))
length(unique(train$Condition2))
length(unique(train$OverallQual))
length(unique(train$OverallCond))
length(unique(train$YearBuilt))
length(unique(train$YearRemodAdd))
length(unique(train$RoofStyle))
length(unique(train$RoofMatl))
length(unique(train$Exterior1st))
length(unique(train$MasVnrType))
length(unique(train$BsmtCond))
length(unique(train$BsmtExposure))
length(unique(train$BsmtFinSF1))
length(unique(train$BsmtFinSF2))
length(unique(train$BsmtUnfSF))
length(unique(train$HeatingQC))
length(unique(train$CentralAir))
length(unique(train$Electrical))
length(unique(train$X1stFlrSF))
length(unique(train$X2ndFlrSF))
length(unique(train$BsmtFullBath))
length(unique(train$FullBath))
length(unique(train$HalfBath))
length(unique(train$KitchenAbvGr))
length(unique(train$KitchenQual))
length(unique(train$Fireplaces))
length(unique(train$GarageType))
length(unique(train$GarageCars))
length(unique(train$GarageArea))
length(unique(train$GarageQual))
length(unique(train$WoodDeckSF))
length(unique(train$EnclosedPorch))
length(unique(train$ScreenPorch))
length(unique(train$PoolArea))
length(unique(train$SaleType))
length(unique(train$SaleCondition))
length(unique(train$Utilities))
# Train
corrplot(cor(train_numeric))
indexesToDrop <- findCorrelation(cor(train_numeric), cutoff = 0.8)
corrplot(cor(train_numeric[,-indexesToDrop]))
train_numeric_cl <- train_numeric[,-indexesToDrop]
# Test
corrplot(cor(test_numeric))
#indexesToDrop <- findCorrelation(cor(test_numeric), cutoff = 0.8)
corrplot(cor(test_numeric[,-indexesToDrop])) # drop the same indexes from the train and test df
test_numeric_cl <- test_numeric[,-indexesToDrop]
summary(lm(log(SalePrice)~.,data=train_numeric_cl))
# Initialize data EDA
train <- train_numeric_cl
summary(train)
lm_all_predictors_log <- lm(log(SalePrice)~.,data=train)
summary(lm_all_predictors_log)
sig_df <- data.frame(summary(lm_all_predictors_log)$coef[summary(lm_all_predictors_log)$coef[,4] <= .05, 4])
sig_df
train <- train_numeric_cl
test <- test_numeric_cl
#regfit.full <- regsubsets(log(SalePrice) ~ . , data = train)
max_vars <- 15
fwd1 <- regsubsets(log(SalePrice)~., data=train, method = "forward", nvmax = max_vars)
par(mfrow=c(2,2))
fwd.summary <- summary(fwd1)
adjr2.max <- which.max(fwd.summary$adjr2)
cp.min <- which.min(fwd.summary$cp)
bic.im <- which.min(fwd.summary$bic)
adjr2.max
cp.min
bic.im
plot(fwd.summary$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
plot(fwd.summary$adjr2,  xlab = "Number of variables", ylab = "Adj R^2", type = "l")
points(adjr2.max, fwd.summary$adjr2[adjr2.max], col = "red", cex = 2, pch = 20)
plot(fwd.summary$cp,  xlab = "Number of variables", ylab = "Cp", type = "l")
points(cp.min, fwd.summary$cp[cp.min], col = "red", cex = 2, pch = 20)
plot(fwd.summary$bic,  xlab = "Number of variables", ylab = "BIC", type = "l")
points(bic.im, fwd.summary$bic[bic.im], col = "red", cex = 2, pch = 20)
plot(fwd1,scale = "r2")
plot(fwd1,scale = "adjr2")
plot(fwd1,scale = "Cp")
plot(fwd1,scale = "bic")
coef(fwd1, max_vars)
#max_vars <- 15
bwd1 <- regsubsets(log(SalePrice)~., data=train, method = "backward", nvmax = max_vars)
par(mfrow=c(2,2))
bwd.summary <- summary(bwd1)
adjr2.max <- which.max(bwd.summary$adjr2)
cp.min <- which.min(bwd.summary$cp)
bic.im <- which.min(bwd.summary$bic)
adjr2.max
cp.min
bic.im
plot(bwd.summary$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
plot(bwd.summary$adjr2,  xlab = "Number of variables", ylab = "Adj R^2", type = "l")
points(adjr2.max, bwd.summary$adjr2[adjr2.max], col = "red", cex = 2, pch = 20)
plot(bwd.summary$cp,  xlab = "Number of variables", ylab = "Cp", type = "l")
points(cp.min, bwd.summary$cp[cp.min], col = "red", cex = 2, pch = 20)
plot(bwd.summary$bic,  xlab = "Number of variables", ylab = "BIC", type = "l")
points(bic.im, bwd.summary$bic[bic.im], col = "red", cex = 2, pch = 20)
plot(bwd1,scale = "r2")
plot(bwd1,scale = "adjr2")
plot(bwd1,scale = "Cp")
plot(bwd1,scale = "bic")
coef(bwd1, max_vars)
#max_vars <- 20
mwd1 <- regsubsets(log(SalePrice)~., data=train, method = "seqrep", nvmax = max_vars)
par(mfrow=c(2,2))
mwd.summary <- summary(mwd1)
adjr2.max <- which.max(mwd.summary$adjr2)
cp.min <- which.min(mwd.summary$cp)
bic.im <- which.min(mwd.summary$bic)
adjr2.max
cp.min
bic.im
plot(mwd.summary$rss, xlab = "Number of variables", ylab = "RSS", type = "l")
plot(mwd.summary$adjr2,  xlab = "Number of variables", ylab = "Adj R^2", type = "l")
points(adjr2.max, mwd.summary$adjr2[adjr2.max], col = "red", cex = 2, pch = 20)
plot(mwd.summary$cp,  xlab = "Number of variables", ylab = "Cp", type = "l")
points(cp.min, mwd.summary$cp[cp.min], col = "red", cex = 2, pch = 20)
plot(mwd.summary$bic,  xlab = "Number of variables", ylab = "BIC", type = "l")
points(bic.im, mwd.summary$bic[bic.im], col = "red", cex = 2, pch = 20)
plot(mwd1,scale = "r2")
plot(mwd1,scale = "adjr2")
plot(mwd1,scale = "Cp")
plot(mwd1,scale = "bic")
coef(mwd1, max_vars)
predict.regsubsets <- function (object, newdata , id, ...){
form <- as.formula(object$call[[2]]) # formula of null model
mat <- model.matrix(form, newdata) # building an "X" matrix from newdata
coefi <- coef(object, id = id) # coefficient estimates associated with the object model containing id non-zero variables
xvars <- names(coefi) # names of the non-zero coefficient estimates
return(mat[,xvars] %*% coefi) # X[,non-zero variables] %*% Coefficients[non-zero variables]
}
# Dataframe creation
coef(fwd1, max_vars)
fwd_vars <- c( "MSSubClass","LotArea","OverallQual","OverallCond","YearBuilt","X1stFlrSF","X2ndFlrSF","BsmtFullBath","TotRmsAbvGrd","Fireplaces","GarageArea","WoodDeckSF","ScreenPorch","BsmtQual","BsmtFinType1","HeatingQC","CentralAir","KitchenQual","PoolQC","SaleCondition","SalePrice")
fwd_vars_test <- c( "MSSubClass","LotArea","OverallQual","OverallCond","YearBuilt","X1stFlrSF","X2ndFlrSF","BsmtFullBath","TotRmsAbvGrd","Fireplaces","GarageArea","WoodDeckSF","ScreenPorch","BsmtQual","BsmtFinType1","HeatingQC","CentralAir","KitchenQual","PoolQC","SaleCondition")
fwd_train_df <- train[fwd_vars]
fwd_test_df <- test[fwd_vars_test]
train <- fwd_train_df
test <- fwd_test_df
nrow(test)
lm_all_predictors_log <- lm(log(SalePrice)~.,data=train)
summary(lm_all_predictors_log)
sig_df <- data.frame(summary(lm_all_predictors_log)$coef[summary(lm_all_predictors_log)$coef[,4] <= .05, 4])
sig_df
par(mfrow=c(2,2))
plot(lm_all_predictors_log)
fwd_train_df <- fwd_train_df[-c(524,1424, 1299, 1171), ]
train <- fwd_train_df
lm_all_predictors_log <- lm(log(SalePrice)~.,data=train)
par(mfrow=c(2,2))
plot(lm_all_predictors_log)
lm.pred1 <- predict(lm_all_predictors_log, newdata = test)
#lm.pred1 <- lm_all_predictors_log %>% predict(test)
length(lm.pred1)
head(lm.pred1)
length(lm.pred1)
## FOR ENETERING INTO KAGGLE
linreg1<-as.data.frame(lm.pred1)
dim(linreg1)
head(linreg1)
linreg1_1<-cbind((1:dim(linreg1)[1]),linreg1)
head(linreg1_1)
colnames(linreg1_1)<-c("Id","SalePrice")
linreg1_1$Id <- linreg1_1$Id + 1460
linreg1_1$SalePrice <- exp(linreg1_1$SalePrice)
dim(linreg1_1)
tail(linreg1_1)
tail(test)
write.csv(linreg1_1, file = "lm_TRIAL.csv", row.names = FALSE)
train <- fwd_train_df
test <- fwd_test_df
X <- model.matrix(log(SalePrice) ~., train)[,-1] # the first column is the intercept
y <- log(train$SalePrice)
ridge.mod <- glmnet(X, y, alpha = 0) # 0 indicates ridge regression
ridge.mod$lambda # it performs ridge regression with these lambdas
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50] # this indicates the coefficent estimates when lambda = bestlam
cv.out <- cv.glmnet(X, y, alpha = 0, nfolds = 10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
coef(ridge.mod, s = bestlam)
# MSE
# Kaggle Predictions
# Create Matrix of Test Data
test.X <- model.matrix(~., test)[,-1] # same form as X
# Predict using ridge regression
ridge.pred <- predict(ridge.mod, s = bestlam, newx = test.X)
# Store predictions as dataframe
ridge_predictions <- as.data.frame(ridge.pred)
# Set up for Kaggle submission
ridge1 <- cbind((1:dim(ridge_predictions)[1]), ridge_predictions)
colnames(ridge1) <- c("Id", "SalePrice")
ridge1$Id <- ridge1$Id +1460
ridge1$SalePrice <- exp(ridge1$SalePrice)
write.csv(ridge1, file = "ridgepredictions.csv", row.names = FALSE)
# Check column count within csv
lasso.mod <- glmnet(X, y, alpha = 1, lambda = grid)
cv.out <- cv.glmnet(X, y, alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
coef(lasso.mod, s = bestlam)
# Predict using Lasso Regression
lasso.pred <- predict(lasso.mod, s = bestlam, newx = test.X)
# Store Predictions as a df
lasso_predictions <- as.data.frame(lasso.pred)
# Set up for Kaggle submission
lasso1 <- cbind((1:dim(lasso_predictions)[1]), lasso_predictions)
colnames(lasso1) <- c("Id", "SalePrice")
lasso1$Id <- lasso1$Id +1460
lasso1$SalePrice <- exp(lasso1$SalePrice)
write.csv(lasso1, file = "lassopredictions.csv", row.names = FALSE)
# Check column count within csv
train <- fwd_train_df
test <- fwd_test_df
tree1 <- rpart(
formula = log(SalePrice) ~ .,
data    = train,
method  = "anova"
)
rpart.plot(tree1)
plotcp(tree1)
tree2 <- rpart(
formula = log(SalePrice) ~ .,
data    = train,
method  = "anova",
control = list(cp = 0, xval = 7.5)
)
plotcp(tree2)
abline(v = 12, lty = "dashed")
# MSE
# Predict using Lasso Regression
reg_tree.pred <- predict(tree2, newdata = test)
# Store Predictions as a df
reg_tree_predictions <- as.data.frame(reg_tree.pred)
# Set up for Kaggle submission
reg_tree1 <- cbind((1:dim(reg_tree_predictions)[1]), reg_tree_predictions)
colnames(reg_tree1) <- c("Id", "SalePrice")
reg_tree1$Id <- reg_tree1$Id +1460
reg_tree1$SalePrice <- exp(reg_tree1$SalePrice)
write.csv(reg_tree1, file = "reg_tree_predictions.csv", row.names = FALSE)
# Check column count within csv
ncol(train)-1 # number of predictors
bag.train <- randomForest(SalePrice ~., data = train, mtry = 20, importance = TRUE)
bag.train
bag.pred <- predict(bag.train, newdata = test)
#table(bag.pred, test$SalePrice)
#mean(bag.pred != train$SalePrice)
importance(bag.train)
varImpPlot(bag.train)
# Create matrix of predictions
bag_predictions <-as.data.frame(bag.pred)
# Process data for submission
bag1<-cbind((1:dim(bag_predictions)[1]),bag_predictions)
colnames(bag1)<-c("Id","SalePrice")
bag1$Id <- bag1$Id + 1460
# Write to csv file for upload
write.csv(bag1, file = "bag_predictions.csv",row.names = FALSE)
# Run LOOCV to select K for KNN
set.seed(1) # Use set.seed(1) for consistency for use of random number generators
# Set train to be our training data for easier typing
# Set cl vector to be output of training data
cl <- log(train$SalePrice)
# Set to be a vector of 100 0's because we will try 100 different K's
cv.error <- rep(0,100)
# Actual function to itterate through data and select best K
for (k in 1:100){
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
cv.error[k] <- mean(pred.class != cl)
}
# Plot our results to see K that produces lowest cv error
plot(1:100, cv.error, type = "b")
# Display K which provides the smallest cv error
which.min(cv.error)
# Create a variable parameterizing code to ensure if data sent changed we could re-produce a result
k_nearest_neighbors <- as.integer(which.min(cv.error))
paste("As we can see from the plot above setting K = ",k_nearest_neighbors,"results in the best prediction when using the KNN algorithm to predict SalesPrice")
# Run knn.reg function since we are using numerical, not categorical data as our response
# We set our cl vector to be the output of the training data (SalesPrice)
cl <- log(train$SalePrice)
# Run KNN regression function
knn_model1 <- knn.reg(train = train[,1:20], test = test[,1:20], y = cl, k = k_nearest_neighbors)
# Plot of predicted values vs. Actual
plot(knn_model1$pred,cl[1:1459],ylab="y", xlab=expression(hat(y)))
# 2 calculations of RMSE to confirm accuracy
RMSE_manual_knn <- sqrt(mean((cl[1:1459] - knn_model1$pred) ^ 2))
RMSE <- RMSE(knn_model1$pred, cl[1:1459])
paste("Our model RMSE is",RMSE)
# True test error csv file creation for KNN
# Create matrix of predictions
knn_predictions <-as.data.frame(exp(knn_model1$pred))
# Process data for submission
knn1<-cbind((1:dim(knn_predictions)[1]),knn_predictions)
colnames(knn1)<-c("Id","SalePrice")
knn1$Id <- knn1$Id + 1460
# Write to csv file for upload
write.csv(knn1, file = "knn_predictions.csv",row.names = FALSE)
train <- fwd_train_df
test <- fwd_test_df
rf.log_SP <- randomForest(log(SalePrice)~.,data=train,mtry=round(sqrt(20)),importance=TRUE,ntree=500,model = T)
rf.log_SP
plot(rf.log_SP)
importance(rf.log_SP)
varImpPlot(rf.log_SP)
prune.rf <- prune.rpart(rf.log_SP, cp=.1)
plot(prune.rf)
prune.rf
# MSE
# Predict using Lasso Regression
rf.pred <- predict(rf.log_SP, newdata = test)
# Store Predictions as a df
rf_predictions <- as.data.frame(rf.pred)
# Set up for Kaggle submission
rf1 <- cbind((1:dim(rf_predictions)[1]), rf_predictions)
colnames(rf1) <- c("Id", "SalePrice")
rf1$Id <- rf1$Id +1460
rf1$SalePrice <- exp(rf1$SalePrice)
write.csv(rf1, file = "rf_predictions.csv", row.names = FALSE)
# Check column count within csv
rpart.plot(tree2)
plot(knn_model1)
knn_model1
summary(knn_model1)
setwd("C:/Users/sam/Desktop/STT465Bayesian/STT465FinalProject")
