summary(lm(y~x +0))
summary(lm(y~x +0))
summary(lm(y~x))
t_new <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
t_new
summary(lm(y~x))
print(summary(lm(y~x)))
print(summary(lm(y~x)))
print(summary(lm(x~y)))
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
data("Weekly")
head(Weekly)
data("Weekly")
head(Weekly)
n <- nrow(Weekly) # Number observations in data set (1089 rows)
n_vec <- seq(1,n, by=1) # Fixed vector containing 1 - 1089
model_calc <- function(Weekly){
#Initialize empty integer (0), everytime prediction is correct add 1 to this
#Empty integer (0) for model i
i_list <- 0
#Empty integer (0) for model i
ii_list <- 0
#for loop from 1 -> n where n is the number of observations in the data set (1089 rows)
for(i in n_vec){
Weekly_working_df <- Weekly[-i,] # Create subset of data removing ith row
#Using new df with i row removed create model i and ii
logit_model_i_weekly_sub <- glm(Direction~Lag1+Lag2, data = Weekly_working_df, family = "binomial")
logit_model_ii_weekly_sub <- glm(Direction~Lag1+Lag2+I(Lag1^2)+I(Lag2^2), data = Weekly_working_df, family = "binomial")
#Posterior Probability Calculation
posterior_prob_i <-  predict.glm(logit_model_i_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
posterior_prob_ii <- predict.glm(logit_model_ii_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
#Get  posterior probability as an integer to use in boolean logic
numeric_i <- as.numeric(posterior_prob_i[1])
numeric_ii <- as.numeric(posterior_prob_ii[1])
#Set prediction to up or down dependant on posterior probability
if(numeric_i>.5){
prediction_i <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(numeric_i<=.5){
prediction_i <- "Down"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)>.5){
prediction_ii <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)<=.5){
prediction_ii <- "Down"
}
#For both models (i & ii) compare to true value and keep a count
true_value_i = Weekly[i,]$Direction
if(prediction_i == true_value_i){
i_list=i_list+1
}
if(prediction_ii == true_value_i){
ii_list=ii_list+1
}
}
print(paste("Count of true for model i is:",i_list))
print(paste("Count of true for model ii is:",ii_list))
#return(i_list)
#return(ii_list)
}
model_calc(Weekly)
i_correct <- 599 #Set given output above, count of times model i was correct
ii_correct <- 590 #Set given output above, count of times model ii was correct
#Compute test errors 1 - (Number of times model correct / row count (n) )
test_error_i <- 1-(i_correct/n)
test_error_ii <- 1-(ii_correct/n)
#Display with easily interpretable notes:
print(paste("The test error for model i is:",test_error_i))
print(paste("The test error for model ii is:",test_error_ii))
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(Weekly)), breaks=10, labels=FALSE)
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
data("College")
set.seed(20)
train <- sample(nrow(College), 600)
College.train <- College[train, ]
College.test <- College[-train, ]
head(College)
college_model1 <- lm(Apps~.,data = College.train)
MSE(College.train$Apps,as.numeric(predict(college_model1)))
X <- model.matrix(~.,data=College.train)
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
X
Y
lasso.mod <- glmnet(X[,-1], Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
dim(X)
dim(Y)
dim(X)
dim(Y)
X <- model.matrix(~.,data=College.train)[,-1]
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = bestlam)
coef(ridge.mod, s = bestlam)
dim(X)
dim(Y)
dimS
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
dim(X)
dim(Test.X)
Test.X
head(X)
head(Test.X)
head(X)
Test.X <- model.matrix(Apps~.,data=College.train)
head(Test.X)
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
head(Test.X)
X <- model.matrix(Apps~.,data=College.train)[,-1]
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = bestlam)
coef(ridge.mod, s = bestlam)
dim(X)
dim(Test.X)
head(X)
head(Test.X)
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
#lasso.pred
library(swirl)
swirl()
colors()
sample(colors(),10)
skip()
skip()
3
3
2
21
2
3
3
21
2
3
3
pal(1)
3
2
21
pal(seq(0,1,len=6))
skip()
p1(2)
p1(6)
skip()
skip()
skip()
p2(10)
p1(20)
showMe(p1(20))
showMe(p2(20))
showMe(p2(2))
p1
?fun
?rgb
1
p3 <- colorRampPalette(c("blue","green"),alpha=.5)
p3
p3(5)
skip()
skip()
skip()
showMe
showMe(cols)
colorRampPalette(cols)
pal <- colorRampPalette(cols)
showMe(pal)
showMe(pal(20))
showMe(pal(20))
image(volcano, col = pal(20))
image(volcano, col = pal(20))
image(volcano, col = p1(20))
str(mpg)
skip()
str(mpg)
qplot(displ, hwy, data = mpg, color = drv)
c()
qplot(displ, hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv)
myhigh
skip()
skip()
skip()
skip()
concat(x)
cacheGenericsMetaData(x)`
(4)
e
sss
for(x in 3){print(x)}
qplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)
skip()
skip()
summary(g)
g+geom_point()
g+geom_point()+geom_smooth()
g+geom_point()+geom_smooth(lm)
g+geom_point()+geom_smooth(string=lm)
g+geom_point()+geom_smooth(method="lm")
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
geom_line)_+ylim(-3,3)
geom_line())_+ylim(-3,3)
geom_line()_+ylim(-3,3)
geom_line()+ylim(-3,3)
g+geom_line()+ylim(-3,3)
g+geom_line()+ylim(c(-3,3)
)
g + geom_line() + coord_cartesian(ylim=c(-3,3))
mpg
g <- ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g +geom_point()
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color=black)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color="black")
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(size=2,method=lm,se=FALSE,color="black")
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black"
)
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black")+labs(x="Displacement",y="Highway Mileage",title="Swirl Rules!")
str(diamonds)
2
22
2
qplot(price,data=diamonds)
range(diamonds$price)
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds,color=cut)
plot(data=carat,price,data=diamonds,color=cut)
plot(data=diamonds,color=cut)
skip()
skip()
2
2
skip()
summary(g)
g+geom_point(alpha=1/3)
summary(g)
cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)
cutpoints
skip()
skip()
skip()
diamonds[myd,]
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smooth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
2
2
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(FNN)
library(leaps)
library(class)
library(glmnet)
library(boot)
library(tidyverse)
library(caret)
reg_best_model <- lm(log(SalePrice)~LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+BsmtFinSF1+BsmtUnfSF+X1stFlrSF+X2ndFlrSF+BsmtFullBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageCars+WoodDeckSF,data=train)
reg_best_model <- lm(log(SalePrice)~LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+BsmtFinSF1+BsmtUnfSF+X1stFlrSF+X2ndFlrSF+BsmtFullBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageCars+WoodDeckSF,data=train)
summary(reg_best_model)
reg_best_model <- lm(log(SalePrice,10)~LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+BsmtFinSF1+BsmtUnfSF+X1stFlrSF+X2ndFlrSF+BsmtFullBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageCars+WoodDeckSF,data=train)
reg_best_model <- lm(log10(SalePrice)~LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+BsmtFinSF1+BsmtUnfSF+X1stFlrSF+X2ndFlrSF+BsmtFullBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageCars+WoodDeckSF,data=train)
train$log_SalePrice <- log10(train$SalePrice)
train$log_SalePrice <- log10(train$SalePrice)
train
train_new
ls
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(FNN)
library(leaps)
library(class)
library(glmnet)
library(boot)
library(tidyverse)
library(caret)
apply(newdf, 2, function(x) any(is.na(x)))
newdf=subset(train_categorical_rp, select=-c(Id,Alley))
# Numeric Value
natomean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
train <- replace(train, TRUE, lapply(train, natomean))
head(train,10)
# Categorical Data Cleaning
train_categorical_rp <- apply(train, 2, function(x){
x[is.na(x)] <- names(which.max(table(x)))
return(x) })
train_categorical_rp
# Numeric Value
natomean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
train <- replace(train, TRUE, lapply(train, natomean))
head(train,10)
# Categorical Data Cleaning
train_categorical_rp <- apply(train, 2, function(x){
x[is.na(x)] <- names(which.max(table(x)))
return(x) })
# Numeric Value
natomean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
train <- replace(train, TRUE, lapply(train, natomean))
#head(train,10)
# Categorical Data Cleaning
train_categorical_rp <- apply(train, 2, function(x){
x[is.na(x)] <- names(which.max(table(x)))
return(x) })
test <- read.csv("Data/test.csv")
train <- read.csv("Data/train.csv")
train
head(train)
#MSSubClass
#MSZoning
train[is.na(train)] <- "No Alley"
sum(is.na(train$MSZoning))
# Numeric Value
natomean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
train <- replace(train, TRUE, lapply(train, natomean))
#head(train,10)
# Categorical Data Cleaning
train_categorical_rp <- apply(train, 2, function(x){
x[is.na(x)] <- names(which.max(table(x)))
return(x) })
#train_categorical_rp
train_categorical_rp <- as.data.frame(train_categorical_rp)
train_categorical_rp
newdf=subset(train_categorical_rp, select=-c(Id,Alley))
#lm(SalePrice~.,data=train_categorical_rp)
apply(newdf, 2, function(x) any(is.na(x)))
#lm(SalePrice~.,data=train_categorical_rp)
apply(newdf, 2, function(x) any(is.na(x)))
lm(SalePrice~Street,data=newdf)
#lm(SalePrice~.,data=train_categorical_rp)
apply(newdf, 2, function(x) any(is.na(x)))
apply(newdf, 2, function(x) type(x))
apply(newdf, 2, function(x) class(x))
lm(log(SalePrice)~Street,data=newdf)
lm(SalePrice~Street,data=newdf)
lm(log(SalePrice)~Street,data=newdf)
lm(log(as.numeric(SalePrice)))~Street,data=newdf)
apply(newdf, 2, function(x) class(x))
knitr::opts_chunk$set(echo = TRUE)
ls
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(FNN)
library(leaps)
library(class)
library(glmnet)
library(boot)
library(tidyverse)
library(caret)
library(compare)
library(randomForest)
rf.log_SP <- randomForest(log(SalePrice)~.,data=train,mtry=round(sqrt(83)),importance=TRUE,ntree=10)
rm(list=ls())
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
read.csv("titanic.csv",header=TRUE)
titanic <- read.csv("titanic.csv",header=TRUE)
na.omit(titanic)
titanic_rm_na <- na.omit(titanic)
titanic <- read.csv("titanic.csv",header=TRUE)
titanic_rm_na <- na.omit(titanic)
head(titanic_rm_na)
titanic.logit <- glm(survived ~ sex+class+age, data = titanic_rm_na, family = "binomial")
titanic <- read.csv("titanic.csv",header=TRUE)
titanic.logit <- glm(survived ~ sex+pclass+age, data = titanic_rm_na, family = "binomial")
summary(titanic.logit)
head(titanic_rm_na)
titanic.logit <- glm(survived ~ sex+pclass+age, data = titanic, family = "binomial")
summary(titanic.logit)
titanic(!is.na())
is.na.data.frame(titanic)
titanic
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
data("Hitters")
Hitters <- Hitters[!is.na(Hitters$Salary),]
set.seed(10)
train <- sample(nrow(Hitters), 200)
Hitters.train <- Hitters[train, ]
Hitters.test <- Hitters[-train, ]
library(leaps)
regfit.full <- regsubsets(Salary ~ . , data = Hitters.train)
summary(regfit.full)
regfit.full <- regsubsets(log(Salary) ~ . , data = Hitters.train)
summary(regfit.full)
regfit.full <- regsubsets(log(Salary) ~ . , data = Hitters, nvmax = 19)
reg.summary <- summary(regfit.full) names(reg.summary)
reg.summary <- summary(regfit.full) names(reg.summary)
reg.summary <- summary(regfit.full)
names(reg.summary)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ", ylab="RSS", type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq", type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables ", ylab = "Cp", type = "l")
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
which.min(reg.summary$cp)
plot(regfit.full,scale="r2"
plot(regfit.full,scale="r2")
plot(regfit.full,scale="r2")
plot(regfit.full,scale="r2")
setwd("C:/Users/sam/Desktop/STT465Bayesian")
getwd()
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
titanic <- read.csv("titanic.csv",header=TRUE)
titanic
titanic_limit <- titanic[["survived","sex","pclass","age"]]
titanic_limit <- titanic["survived","sex","pclass","age"]
library(dplyr)
titanic %>% select(survived,sex,pclass, age)
titanic_limit <- titanic %>% select(survived,sex,pclass, age)
titanic_rm_na <- na.omit(titanic_limit)
head(titanic_rm_na)
titanic.logit <- glm(survived ~ sex+pclass+age, data = titanic_rm_na, family = "binomial")
summary(titanic.logit)
summary(titanic.logit)
