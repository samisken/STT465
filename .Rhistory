library(glmnet)
library(MLmetrics)
#library(swirl)
data("Weekly")
head(Weekly)
data("Weekly")
head(Weekly)
n <- nrow(Weekly) # Number observations in data set (1089 rows)
n_vec <- seq(1,n, by=1) # Fixed vector containing 1 - 1089
model_calc <- function(Weekly){
#Initialize empty integer (0), everytime prediction is correct add 1 to this
#Empty integer (0) for model i
i_list <- 0
#Empty integer (0) for model i
ii_list <- 0
#for loop from 1 -> n where n is the number of observations in the data set (1089 rows)
for(i in n_vec){
Weekly_working_df <- Weekly[-i,] # Create subset of data removing ith row
#Using new df with i row removed create model i and ii
logit_model_i_weekly_sub <- glm(Direction~Lag1+Lag2, data = Weekly_working_df, family = "binomial")
logit_model_ii_weekly_sub <- glm(Direction~Lag1+Lag2+I(Lag1^2)+I(Lag2^2), data = Weekly_working_df, family = "binomial")
#Posterior Probability Calculation
posterior_prob_i <-  predict.glm(logit_model_i_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
posterior_prob_ii <- predict.glm(logit_model_ii_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
#Get  posterior probability as an integer to use in boolean logic
numeric_i <- as.numeric(posterior_prob_i[1])
numeric_ii <- as.numeric(posterior_prob_ii[1])
#Set prediction to up or down dependant on posterior probability
if(numeric_i>.5){
prediction_i <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(numeric_i<=.5){
prediction_i <- "Down"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)>.5){
prediction_ii <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)<=.5){
prediction_ii <- "Down"
}
#For both models (i & ii) compare to true value and keep a count
true_value_i = Weekly[i,]$Direction
if(prediction_i == true_value_i){
i_list=i_list+1
}
if(prediction_ii == true_value_i){
ii_list=ii_list+1
}
}
print(paste("Count of true for model i is:",i_list))
print(paste("Count of true for model ii is:",ii_list))
#return(i_list)
#return(ii_list)
}
model_calc(Weekly)
i_correct <- 599 #Set given output above, count of times model i was correct
ii_correct <- 590 #Set given output above, count of times model ii was correct
#Compute test errors 1 - (Number of times model correct / row count (n) )
test_error_i <- 1-(i_correct/n)
test_error_ii <- 1-(ii_correct/n)
#Display with easily interpretable notes:
print(paste("The test error for model i is:",test_error_i))
print(paste("The test error for model ii is:",test_error_ii))
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(Weekly)), breaks=10, labels=FALSE)
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
data("College")
set.seed(20)
train <- sample(nrow(College), 600)
College.train <- College[train, ]
College.test <- College[-train, ]
head(College)
college_model1 <- lm(Apps~.,data = College.train)
MSE(College.train$Apps,as.numeric(predict(college_model1)))
X <- model.matrix(~.,data=College.train)
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
X
Y
lasso.mod <- glmnet(X[,-1], Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
dim(X)
dim(Y)
dim(X)
dim(Y)
X <- model.matrix(~.,data=College.train)[,-1]
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = bestlam)
coef(ridge.mod, s = bestlam)
dim(X)
dim(Y)
dimS
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
dim(X)
dim(Test.X)
Test.X
head(X)
head(Test.X)
head(X)
Test.X <- model.matrix(Apps~.,data=College.train)
head(Test.X)
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
head(Test.X)
X <- model.matrix(Apps~.,data=College.train)[,-1]
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = bestlam)
coef(ridge.mod, s = bestlam)
dim(X)
dim(Test.X)
head(X)
head(Test.X)
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
#lasso.pred
library(swirl)
swirl()
colors()
sample(colors(),10)
skip()
skip()
3
3
2
21
2
3
3
21
2
3
3
pal(1)
3
2
21
pal(seq(0,1,len=6))
skip()
p1(2)
p1(6)
skip()
skip()
skip()
p2(10)
p1(20)
showMe(p1(20))
showMe(p2(20))
showMe(p2(2))
p1
?fun
?rgb
1
p3 <- colorRampPalette(c("blue","green"),alpha=.5)
p3
p3(5)
skip()
skip()
skip()
showMe
showMe(cols)
colorRampPalette(cols)
pal <- colorRampPalette(cols)
showMe(pal)
showMe(pal(20))
showMe(pal(20))
image(volcano, col = pal(20))
image(volcano, col = pal(20))
image(volcano, col = p1(20))
str(mpg)
skip()
str(mpg)
qplot(displ, hwy, data = mpg, color = drv)
c()
qplot(displ, hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv)
myhigh
skip()
skip()
skip()
skip()
concat(x)
cacheGenericsMetaData(x)`
(4)
e
sss
for(x in 3){print(x)}
qplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)
skip()
skip()
summary(g)
g+geom_point()
g+geom_point()+geom_smooth()
g+geom_point()+geom_smooth(lm)
g+geom_point()+geom_smooth(string=lm)
g+geom_point()+geom_smooth(method="lm")
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
geom_line)_+ylim(-3,3)
geom_line())_+ylim(-3,3)
geom_line()_+ylim(-3,3)
geom_line()+ylim(-3,3)
g+geom_line()+ylim(-3,3)
g+geom_line()+ylim(c(-3,3)
)
g + geom_line() + coord_cartesian(ylim=c(-3,3))
mpg
g <- ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g +geom_point()
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color=black)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color="black")
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(size=2,method=lm,se=FALSE,color="black")
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black"
)
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black")+labs(x="Displacement",y="Highway Mileage",title="Swirl Rules!")
str(diamonds)
2
22
2
qplot(price,data=diamonds)
range(diamonds$price)
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds,color=cut)
plot(data=carat,price,data=diamonds,color=cut)
plot(data=diamonds,color=cut)
skip()
skip()
2
2
skip()
summary(g)
g+geom_point(alpha=1/3)
summary(g)
cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)
cutpoints
skip()
skip()
skip()
diamonds[myd,]
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smooth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
2
2
setwd("C:/Users/sam/Desktop/STT465Bayesian")
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(coda)
WAGES <- read.table("wages.txt",header = TRUE)
head(WAGES)
pairs(WAGES)
summary(WAGES)
range(WAGES)
sapply(WAGES, range)
pairs(WAGES)
pairs(WAGES)
summary(WAGES)
sapply(WAGES, range)
pairs(WAGES)
summary(WAGES)
sapply(WAGES, range)
plot(WAGES$Education)
plot(count(WAGES$Education))
lin_model_1 <- lm(Wage~Education+Black+Hispanic+Experience+South+Sex+Married,data=WAGES)
summary(lin_model_1)
### Gibbs Sampler #######
gibbsMLR=function(y,X,nIter=10000,df0=4,S0=var(y)*0.8*(df0-2),b0=0,varB=1e12,verbose=500){
## Objects to store samples
p=ncol(X); n=nrow(X)
B=matrix(nrow=nIter,ncol=p,0) # create a matrix to store the gibbs sample for beta
varE=rep(NA,nIter)      # .. for error variance
## Initialize
B[1,]=0     # initial values for slopes
B[1,1]=mean(y)  # initial value for y-intercept
b=B[1,]
varE[1]=var(y)  # initial error variance
resid=y-B[1,1]  # centered y (orthogonal)
## Computing sum x'x for each column
SSx=colSums(X^2)
for(i in 2:nIter){
# Sampling regression coefficients
for(j in 1:p){
A=SSx[j]/varE[i-1]+1/varB
Xy= sum(X[,j]*resid)+SSx[j]*b[j]  # equivalent to X[,j]'(y-X[,-j]%*%b[-j])
rhs=Xy/varE[i-1]  + b0/varB  # Numerator of beta^tilda_k
condMean=rhs/A
condVar=1/A
b_old=b[j]
b[j]=rnorm(n=1,mean=condMean,sd=sqrt(condVar))
B[i,j]=b[j]
resid=resid-X[,j]*(b[j]-b_old) # updating residuals
}
# Sampling the error variance
RSS=sum(resid^2)
DF=n+df0
S=RSS+S0
varE[i]=S/rchisq(df=DF,n=1)
## if(i%%verbose==0){ cat(' Iter ', i, '\n') }
}
out=list(effects=B,varE=varE)
return(out)
}
# Data Pre-Processing and Sampling
wage <- WAGES$Wage
X <- model.matrix(~Education+Black+Hispanic+Experience+South+Sex+Married,data=WAGES)
SAMPLES <- gibbsMLR(y=wage,X=X,nIter = 15000)
dim(SAMPLES$effects) #Columns = effects, rows = Samples
head(SAMPLES$varE)
plot(SAMPLES$varE[1:500],type='o')
plot(SAMPLES$effects[1:500,1],type='o')
plot(SAMPLES$effects[1:5000,2],type='o')
dim(SAMPLES$effects) #Columns = effects, rows = Samples
head(SAMPLES$varE)
plot(SAMPLES$varE[1:500],type='o')
plot(SAMPLES$effects[1:500,1],type='o')
plot(SAMPLES$effects[1:500,1],type='o')
plot(SAMPLES$effects[1:5000,2],type='o')
plot(SAMPLES$varE[1:500],type='o')
plot(SAMPLES$effects[1:500,1],type='o')
# Trace plot of all variables
plot(SAMPLES$varE[1:500],type='o')
# Intercept trace plot
plot(SAMPLES$effects[1:500,1],type='o')
# The plot below allows us to decide on burn-in quantity, since the data centers around index = 1000 I will burn-n (remove) the first 1000 samples
plot(SAMPLES$effects[1:5000,2],type='o')
# Discarding Burn - In
B <- SAMPLES$effects[-(1:1000),];colnames(B)=colnames(X)
varE <- SAMPLES$varE[-(1:1000)]
summary(B)
# Convert to a MCMC object
B <- as.mcmc(B)
varE <- as.mcmc(varE)
autocorr.plot(B[,1],lag.max=100)
autocorr.plot(B[,2],lag.max=100)
summary(B)
summary(varE)
HPDinterval.mcmc(B, prob=.95)
knitr::opts_chunk$set(echo = TRUE)
library(coda)
summary(B)
summary(varE)
HPDinterval.mcmc(B, prob=.95)
HPDinterval <- function(obj, prob = 0.95, ...) UseMethod("HPDinterval")
HPDinterval.mcmc <- function(obj, prob = 0.95, ...)
{
obj <- as.matrix(obj)
vals <- apply(obj, 2, sort)
if (!is.matrix(vals)) stop("obj must have nsamp > 1")
nsamp <- nrow(vals)
npar <- ncol(vals)
gap <- max(1, min(nsamp - 1, round(nsamp * prob)))
init <- 1:(nsamp - gap)
inds <- apply(vals[init + gap, ,drop=FALSE] - vals[init, ,drop=FALSE],
2, which.min)
ans <- cbind(vals[cbind(inds, 1:npar)],
vals[cbind(inds + gap, 1:npar)])
dimnames(ans) <- list(colnames(obj), c("lower", "upper"))
attr(ans, "Probability") <- gap/nsamp
ans
}
HPDinterval.mcmc.list <- function(obj, prob = 0.95, ...)
lapply(obj, HPDinterval, prob)
summary(B)
summary(varE)
HPDinterval.mcmc(B, prob=.95)
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%%contrast
plot(density(tmp));abline(v=0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
mean(tmp>0)
mean(tmp<0)
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
mean(tmp>0)
mean(tmp<0)
sd(tmp<0)
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
mean(tmp>0)
mean(tmp<0)
sd(tmp>0)
sd(tmp<0)
print(paste(HPDinterval.mcmc(B, prob=.95), "is the posterior credibility interval")
summary(B)
summary(B)
summary(varE)
print("The below shows the poterior credibility interval")
HPDinterval.mcmc(B, prob=.95)
summary(B)
summary(varE)
print("The below shows the poterior credibility interval")
HPDinterval.mcmc(B, prob=.95)
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
mean(tmp>0)
mean(tmp<0)
contrast=c(0,1,-1,0,0,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
mean(tmp>0)
mean(tmp<0)
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
mean(tmp>0)
mean(tmp<0)
print(paste("The ethnic disparity between black and hispanic is > 0", mean(tmp>0)/100, "% and < 0",mean(tmp<0)/100,%))
print(paste("The ethnic disparity between black and hispanic is > 0", mean(tmp>0)/100))#, "% and < 0", mean(tmp<0)/100,%))
print(paste("The ethnic disparity between black and hispanic is > 0", mean(tmp>0)*100))#, "% and < 0", mean(tmp<0)/100,%))
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
print(paste("The ethnic disparity between black and hispanic is > 0", mean(tmp>0)*100))
print(paste("The ethnic disparity between black and hispanic is< 0", mean(tmp<0)/100))
print("Both in percent of time")
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
plot(density(tmp));abline(v=0)
abline(v=HPDinterval.mcmc(as.mcmc(tmp),p=.95),col=2,lty=2)
print(paste("The ethnic disparity between black and hispanic is > 0", mean(tmp>0)*100))
print(paste("The ethnic disparity between black and hispanic is< 0", mean(tmp<0)*100))
print("Both in percent of time")
