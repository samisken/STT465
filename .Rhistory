accept[1,]=TRUE
# Initialize
B[1,]=0
B[1,1]=log(mean(y)/(1-mean(y)))
b=B[1,]
#print(b) # Test
for(i in 2:nIter){
for(j in 1:p){
candidate=b
candidate[j]=rnorm(mean=b[j],sd=sqrt(V),n=1)
logP_current=logP(y,X,b0=b0,varB=varB,b=b)
logP_candidate=logP(y,X,b0=b0,varB=varB,b=candidate)
r=min(1,exp(logP_candidate-logP_current))
delta=rbinom(n=1,size=1,p=r)
accept[i,j]=delta
if(delta==1){ b[j]=candidate[j] }
}
B[i,]=b
if(i%%1000==0){
message(" Iteration ",i)
}
}
return(list(B=B,accept=accept))
}
#y: titanic_rm_na$survived
#X: cbind(as.matrix(model.matrix(~survived+sex+pclass+age,data=titanic_rm_na)))
Z=as.matrix(model.matrix(~sex+pclass+age,data=titanic_rm_na))#[,-1]
samples=logisticRegressionBayes(y=titanic_rm_na$survived,X=cbind(Z),nIter=55000)
samples_df <- as.data.frame(samples)
head(samples_df,10)
burn_in <- 5000
samples_post_burn_in <- tail(samples_df, -burn_in)
head(samples_post_burn_in)
nrow(samples_post_burn_in)
library(coda)
samples_mcmc <- as.mcmc(samples_post_burn_in)
# Clearly we have stationarity
autocorr.plot(samples_mcmc,lag.max = 100)
# HPD Interval
HPDinterval(samples_mcmc,prob = .95)
summary(samples_mcmc)
traceplot(samples_mcmc)
effectiveSize(samples_mcmc)
summary(samples_mcmc)
densplot(samples_mcmc)
# V=.5, V=.1,V=.001,V=.0001, and V=.00005
samples1=logisticRegressionBayes(y=titanic_rm_na$survived,X=cbind(Z),nIter=55000,V=.5)
samples2=logisticRegressionBayes(y=titanic_rm_na$survived,X=cbind(Z),nIter=55000,V=.1)
samples3=logisticRegressionBayes(y=titanic_rm_na$survived,X=cbind(Z),nIter=55000,V=.001)
samples4=logisticRegressionBayes(y=titanic_rm_na$survived,X=cbind(Z),nIter=55000,V=.0001)
samples5=logisticRegressionBayes(y=titanic_rm_na$survived,X=cbind(Z),nIter=55000,V=.00005)
samples1df=as.data.frame(samples1)
samples2df=as.data.frame(samples2)
samples3df=as.data.frame(samples3)
samples4df=as.data.frame(samples4)
samples5df=as.data.frame(samples5)
autocorr(as.mcmc(as.data.frame(samples1)),lag=50)
autocorr(as.mcmc(as.data.frame(samples2)),lag=50)
autocorr(as.mcmc(as.data.frame(samples3)),lag=50)
autocorr(as.mcmc(as.data.frame(samples4)),lag=50)
autocorr(as.mcmc(as.data.frame(samples5)),lag=50)
effectiveSize(as.mcmc(as.data.frame(samples1)))
effectiveSize(as.mcmc(as.data.frame(samples2)))
effectiveSize(as.mcmc(as.data.frame(samples3)))
effectiveSize(as.mcmc(as.data.frame(samples4)))
effectiveSize(as.mcmc(as.data.frame(samples5)))
rm(list=ls())
rm(list=ls())
setwd("C:/Users/sam/Desktop/STT465Bayesian")
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(ggplot2)
library(coda)
library(rjags)
library(ggplot2)
library(corrplot)
data("gavote")
summary(gavote)
head(gavote)
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(ggplot2)
library(coda)
library(rjags)
library(ggplot2)
library(corrplot)
data("gavote")
summary(gavote)
head(gavote)
head(gavote)
# Data Cleaning
# Check for na values
gavote <- na.omit(gavote)
#Re-Encode rural and Atlanta
# rural
unique(gavote$rural)
gavote$rural <- ifelse(gavote$rural == 'rural',1,0)
# Atlanta
unique(gavote$atlanta)
gavote$atlanta <- ifelse(gavote$atlanta == 'Atlanta',1,0)
pairs(gavote)
ggplot(gavote, aes(x=undercount)) + geom_histogram(bins=45) + stat_function(fun = dnorm, args = list(mean = mean(gavote$undercount), sd=sd(gavote$undercount)))
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(ggplot2)
library(coda)
library(rjags)
library(ggplot2)
library(corrplot)
data("gavote")
summary(gavote)
head(gavote)
help(gavote)
summary(gavote)
gavote$undercount<- (gavote$ballots - gavote$votes)/gavote$ballots
print(paste("The range of the undercount (in percent of county votes) is:",range(gavote$undercount)[1]," , ",range(gavote$undercount)[2]))
print(paste("The mean of undercount (in percent of county votes) is:",mean(gavote$undercount)))
upper <- max(gavote$undercount)
print(paste("The max value of undercount (in percent of county of county votes) is: " , upper))
Georgia_County_Level_Results <- read.csv("Georgia_County_Level_Results.csv")
Georgia_County_Level_Results[] <- lapply(Georgia_County_Level_Results, as.numeric)
Georgia_County_Level_Results$Percent_Dif <- Georgia_County_Level_Results$Bush_Percent - Georgia_County_Level_Results$Gore_Percent
Georgia_County_Level_Results$within_margin_of_undercount <- ifelse(Georgia_County_Level_Results$Percent_Dif<=upper,1,0)
percent_counties_within_undercount_margin <- sum(Georgia_County_Level_Results$within_margin_of_undercount)/nrow(Georgia_County_Level_Results)
head(Georgia_County_Level_Results)
# Visualizations
ggplot(Georgia_County_Level_Results, aes(x=Percent_Dif)) + geom_histogram(bins=45,color="blue", fill="red") + geom_vline(aes(xintercept=mean(Percent_Dif)),color="blue", linetype="dashed", size=1)+
labs(title="Distribution of Undercount (Percent of County Vote)",x="Percent of County Vote", y = "Count")
print(paste(percent_counties_within_undercount_margin,"were within margin of error max (18%)"))
head(gavote)
# Data Cleaning
# Check for na values
gavote <- na.omit(gavote)
#Re-Encode rural and Atlanta
# rural
unique(gavote$rural)
gavote$rural <- ifelse(gavote$rural == 'rural',1,0)
# Atlanta
unique(gavote$atlanta)
gavote$atlanta <- ifelse(gavote$atlanta == 'Atlanta',1,0)
pairs(gavote)
ggplot(gavote, aes(x=undercount)) + geom_histogram(bins=45) + stat_function(fun = dnorm, args = list(mean = mean(gavote$undercount), sd=sd(gavote$undercount)))
shapiro.test(gavote$undercount)
# Distribution of Categorical Variables
g <- ggplot(gavote, aes(econ))
g + geom_bar(aes(fill=econ), width = 0.5) +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Histogram on Economic Status of Georgia Counties",
subtitle="gavote")
q <- ggplot(gavote, aes(equip))
q + geom_bar(aes(fill=equip), width = 0.5) +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Histogram on Voting Equipment used by Georgia Counties",
subtitle="gavote")
# Histogram on a Categorical variable
r <- ggplot(gavote, aes(econ))
r + geom_bar(aes(fill=equip), width = 0.5) +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Histogram Showing Relationship Between Economic Status and Voting Equipment",
subtitle="Voting Equipment Across Economic Status")
stepwise_lm_1 <- lm(undercount~equip+econ+perAA+rural+atlanta+gore+bush+other+votes+ballots,data=gavote)
summary(stepwise_lm_1)
stepwise_lm_2 <- lm(undercount~equip+econ+gore+bush+ballots,data=gavote)
summary(stepwise_lm_2)
par(mfrow=c(2,2))
plot(stepwise_lm_2)
abline(stepwise_lm_2)
# Remove Fulton
remove <- c("FULTON")
gavote <- gavote[!(row.names(gavote) %in% remove), ]
stepwise_lm_2 <- lm(undercount~equip+econ+gore+bush+ballots,data=gavote)
summary(stepwise_lm_2)
par(mfrow=c(2,2))
plot(stepwise_lm_2)
abline(stepwise_lm_2)
## Prediction equation coefficient
bHat <- coef(stepwise_lm_2)
bHat
# OLS fit (MLR) Scatterplot Comparison
pairs(gavote) # Scatterplot matrix for ALL variables
# Correlation Matrix for Model  with Stepwise Variable Reduction Performed
gavote1 <- gavote[-c(3,4,5,8,9)]
pairs(gavote1)
## Prediction equations
bHat1=coef(stepwise_lm_2)
bHat1
# Prediction Equation for: a poor county, with 3000 votes for Gore, 2000 votes for Bush, lever voting (0 for all) and 5500 ballots
Int= bHat1[1]+bHat1[6]+bHat1[6]+bHat1[8]+10*bHat1[9]+bHat1[10]
Slope=bHat1[2]
Edu=seq(from=5,to=18,by=1)
lines(x=Edu,y=Int+Edu*Slope,col=4)
##########################################################
##########################################################
################## OLS Using Matrix  #####################
# Incidence matrix for intercept and effects of Educ, South, Black,
## Hisp, Sex, Married, Exp
X=cbind(1,gavote$equip,gavote$econ, gavote$gore, gavote$bush, gavote$ballots)
X1 <- model.matrix( ~equip+econ+gore+bush+ballots,data=gavote)
head(X)
y=gavote$undercount
head(y)
dim (X)
y = as.matrix(y)
dim(y)
# OLS equations
Xy=t(X)%*%y
XtX=t(X)%*%X
# Estimates, compare with fm$coef
bHat2=solve(XtX,Xy)
bHat2
# To get SEs we need an estimate of the error variance
fitV = X%*%bHat2  ## Fitted values from ourmodel
eHat=y-fitV     ### Residual
plot(eHat)        ## Sequence plot of residual
#plot(eHat ~ WAGES$Education) ### ehat vs x
plot(eHat ~ fitV)   ### Residual vs fitted value plot
vEHat=sum(eHat^2)/(nrow(gavote)-ncol(X)) # Sum of squares of errors divide by n-rank(X)
SE=sqrt(diag(solve(XtX)*vEHat)) ## SE of bHat
t_stat=bHat2/SE
## P-values (under normality first, and under t then...)
pvaluesN=pnorm(abs(t_stat),lower.tail=F)*2
pvaluesT=pt(abs(t_stat),df=length(y)-ncol(X),lower.tail=F)*2
# Regression Outputs
summary(stepwise_lm_2)
cbind(bHat2,SE,t_stat,pvaluesT)
########  OLS fit (Frequentist Inference)
summary(stepwise_lm_2)
## Coefficients of Prediction equations
bHat= coef(stepwise_lm_2)
bHat
### Gibbs sampler for a Muliple Linear Regression Model (MLR) ###
####### Bayesian Inference ###########
##  y (nx1) response  ###
### X (nxp) the covariate matrix for effects  ###
### nIter, the number of samples to be collected, set by default to 10,000.
##  Hyper-parameters: df0 and S0 (the scale and degree of freedom for the scaled-inverse chi-square prior)
##                  : b0 and varB (the mean and variance of the normal prior assigned to effects).
gibbsMLR=function(y,X,nIter=10000,df0=4,S0=var(y)*0.8*(df0-2),b0=0,varB=1e12,verbose=500){
## Objects to store samples
p=ncol(X); n=nrow(X)
B=matrix(nrow=nIter,ncol=p,0) # create a matrix to store the gibbs sample for beta
varE=rep(NA,nIter)      # .. for error variance
## Initialize
B[1,]=0     # initial values for slopes
B[1,1]=mean(y)  # initial value for y-intercept
b=B[1,]
varE[1]=var(y)  # initial error variance
resid=y-B[1,1]  # centered y (orthogonal)
## Computing sum x'x for each column
SSx=colSums(X^2)
for(i in 2:nIter){
# Sampling regression coefficients
for(j in 1:p){
A=SSx[j]/varE[i-1]+1/varB
Xy= sum(X[,j]*resid)+SSx[j]*b[j]  # equivalent to X[,j]'(y-X[,-j]%*%b[-j])
rhs=Xy/varE[i-1]  + b0/varB  # Numerator of beta^tilda_k
condMean=rhs/A
condVar=1/A
b_old=b[j]
b[j]=rnorm(n=1,mean=condMean,sd=sqrt(condVar))
B[i,j]=b[j]
resid=resid-X[,j]*(b[j]-b_old) # updating residuals
}
# Sampling the error variance
RSS=sum(resid^2)
DF=n+df0
S=RSS+S0
varE[i]=S/rchisq(df=DF,n=1)
## if(i%%verbose==0){ cat(' Iter ', i, '\n') }
}
out=list(effects=B,varE=varE)
return(out)
}
transformed_undercount=gavote$transformed_undercount
X=model.matrix(~.,data=gavote1)
SAMPLES=gibbsMLR(y=transformed_undercount,X=X,nIter=15000)
undercount=gavote$undercount
X=model.matrix(~.,data=gavote1)
SAMPLES=gibbsMLR(y=undercount,X=X,nIter=15000)
dim(SAMPLES$effects)# cols are effects, rows are samples
head(SAMPLES$varE)
# trace plots of a few parameters....
plot(SAMPLES$varE[1:500],type='o') # 1st 100 interations should be discared as burn in
plot(SAMPLES$effects[1:500,1],type='o') #Intercept, probably 1st 200 needs to be discarded
plot(SAMPLES$effects[1:5000,2],type='o') # effect of education. Discard 2000?
## Discarding 500 iterations as burn-in
B=SAMPLES$effects[-(1:500),];colnames(B)=colnames(X)
varE=SAMPLES$varE[-(1:500)]
summary(B)
# Converting to MCMC object
B=as.mcmc(B)
varE=as.mcmc(varE)
autocorr.plot(B[,1],lag.max=100)
autocorr.plot(B[,2],lag.max=100)
summary(B)
summary(varE)
HPDinterval(B,prob=.95)
######## Bayesian Linear Hypothesis  #######
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
B
contrast=c(0,0,0,1,-1,0,0,0)
contrast=as.mcmc(c(0,0,0,1,-1,0,0,0))
tmp=B%*%contrast
contrast=c(0,0,0,1,-1,0,0,0)
# The plot below allows us to decide on burn-in quantity, since the data centers around index = 1000 I will burn-n (remove) the first 1000 samples
plot(SAMPLES$effects[1:5000,2],type='o') # effect of education. Discard 2000?
# Intercept trace plot
plot(SAMPLES$effects[1:500,1],type='o') #Intercept, probably 1st 200 needs to be discarded
# Trace plot of all variables
plot(SAMPLES$varE[1:500],type='o') # 1st 100 interations should be discared as burn in
# The plot below allows us to decide on burn-in quantity, since the data centers around index = 500 I will burn-n (remove) the first 500 samples
plot(SAMPLES$effects[1:5000,2],type='o') # effect of education. Discard 2000?
autocorr.plot(B[,1],lag.max=100)
autocorr.plot(B[,2],lag.max=100)
summary(B)
summary(varE)
HPDinterval(B,prob=.95)
######## Bayesian Linear Hypothesis  #######
contrast=c(0,0,0,1,-1,0,0,0)
tmp=B%*%contrast
autocorr.plot(B[,1],lag.max=100)
autocorr.plot(B[,2],lag.max=100)
HPDinterval <- function(obj, prob = 0.95, ...) UseMethod("HPDinterval")
HPDinterval.mcmc <- function(obj, prob = 0.95, ...)
{
obj <- as.matrix(obj)
vals <- apply(obj, 2, sort)
if (!is.matrix(vals)) stop("obj must have nsamp > 1")
nsamp <- nrow(vals)
npar <- ncol(vals)
gap <- max(1, min(nsamp - 1, round(nsamp * prob)))
init <- 1:(nsamp - gap)
inds <- apply(vals[init + gap, ,drop=FALSE] - vals[init, ,drop=FALSE],
2, which.min)
ans <- cbind(vals[cbind(inds, 1:npar)],
vals[cbind(inds + gap, 1:npar)])
dimnames(ans) <- list(colnames(obj), c("lower", "upper"))
attr(ans, "Probability") <- gap/nsamp
ans
}
HPDinterval.mcmc.list <- function(obj, prob = 0.95, ...)
lapply(obj, HPDinterval, prob)
# source: https://github.com/cran/coda/blob/master/R/HPDinterval.R
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(ggplot2)
library(coda)
library(rjags)
library(ggplot2)
library(corrplot)
# Distribution of Categorical Variables
g <- ggplot(gavote, aes(econ))
g + geom_bar(aes(fill=econ), width = 0.5) +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Histogram on Economic Status of Georgia Counties",
subtitle="gavote")
q <- ggplot(gavote, aes(equip))
q + geom_bar(aes(fill=equip), width = 0.5) +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Histogram on Voting Equipment used by Georgia Counties",
subtitle="gavote")
# Histogram on a Categorical variable
r <- ggplot(gavote, aes(econ))
r + geom_bar(aes(fill=equip), width = 0.5) +
theme(axis.text.x = element_text(angle=65, vjust=0.6)) +
labs(title="Histogram Showing Relationship Between Economic Status and Voting Equipment",
subtitle="Voting Equipment Across Economic Status")
stepwise_lm_1 <- lm(undercount~equip+econ+perAA+rural+atlanta+gore+bush+other+votes+ballots,data=gavote)
summary(stepwise_lm_1)
stepwise_lm_2 <- lm(undercount~equip+econ+gore+bush+ballots,data=gavote)
summary(stepwise_lm_2)
par(mfrow=c(2,2))
plot(stepwise_lm_2)
abline(stepwise_lm_2)
# Prediction Equation for: a poor county, with 3000 votes for Gore, 2000 votes for Bush, lever voting (0 for all) and 5500 ballots
Int= bHat1[1]+bHat1[6]+bHat1[6]+bHat1[8]+10*bHat1[9]+bHat1[10]
Slope=bHat1[2]
lines(x=pg,y=Int+pg*Slope,col=4)
pg=seq(from=5,to=18,by=1)
lines(x=pg,y=Int+pg*Slope,col=4)
plot.new()
lines(x=pg,y=Int+pg*Slope,col=4)
plot.new()
lines(x=pg,y=Int+pg*Slope,col=4)
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(ggplot2)
library(coda)
library(rjags)
library(ggplot2)
library(corrplot)
# OLS fit (MLR) Scatterplot Comparison
pairs(gavote) # Scatterplot matrix for ALL variables
# Correlation Matrix for Model  with Stepwise Variable Reduction Performed
gavote1 <- gavote[-c(3,4,5,8,9)]
pairs(gavote1)
bHat1
# Prediction Equation for: a poor county, with 3000 votes for Gore, 2000 votes for Bush, lever voting (0 for all) and 5500 ballots
Int= bHat1[1]+bHat1[6]+bHat1[6]+bHat1[8]+10*bHat1[9]+bHat1[10]
INT
Int
# Prediction Equation for: a poor county, with 3000 votes for Gore, 2000 votes for Bush, lever voting (0 for all) and 5500 ballots
Int= bHat1[1]+bHat1[6]+bHat1[6]+bHat1[8]+3000*bHat1[9]+2000*bHat1[10]+5500*bHat1[11]
# Prediction Equation for: a poor county, with 3000 votes for Gore, 2000 votes for Bush, lever voting (0 for all) and 5500 ballots
Int= bHat1[1]+bHat1[6]+bHat1[6]+bHat1[8]+3000*bHat1[9]
Slope=2000*bHat1[10]+5500*bHat1[11]
pg=seq(from=5,to=18,by=1)
Int
Slope
plot(Slope)
plot(Int)
## Prediction equations
bHat1=coef(stepwise_lm_2)
bHat1
y1=0+2.898298e-02+2.089696e-02+(3000*-3.554025e-05) +(2000*-3.386005e-05)+(5500*3.282120e-05)
print(paste("There is a predicted value",y1*10,"% of votes to be undercounted by in this county"))
print(paste("There is a predicted value",y1*100,"% of votes to be undercounted by in this county"))
# Prediction Equation for: a poor county, with 2000 votes for Gore, 3000 votes for Bush, lever voting (0 for all) and 5500 ballots
y2=0+2.898298e-02+2.089696e-02+(2000*-3.554025e-05) +(3000*-3.386005e-05)+(5500*3.282120e-05)
print(paste("There is a predicted value",y2*100,"% of votes to be undercounted by in this county"))
# Prediction Equation for: a RICH county, with 2000 votes for Gore, 3000 votes for Bush, lever voting (0 for all) and 5500 ballots
y1=0+2.898298e-02+-1.894076e-02+(3000*-3.554025e-05) +(2000*-3.386005e-05)+(5500*3.282120e-05)
print(paste("There is a predicted value",y3*100,"% of votes to be undercounted by in this county"))
# Prediction Equation for: a RICH county, with 2000 votes for Gore, 3000 votes for Bush, lever voting (0 for all) and 5500 ballots
y3=0+2.898298e-02+-1.894076e-02+(3000*-3.554025e-05) +(2000*-3.386005e-05)+(5500*3.282120e-05)
print(paste("There is a predicted value",y3*100,"% of votes to be undercounted by in this county"))
##########################################################
##########################################################
################## OLS Using Matrix  #####################
# Incidence matrix for intercept and effects of Educ, South, Black,
## Hisp, Sex, Married, Exp
X=cbind(1,gavote$equip,gavote$econ, gavote$gore, gavote$bush, gavote$ballots)
X1 <- model.matrix( ~equip+econ+gore+bush+ballots,data=gavote)
head(X)
y=gavote$undercount
head(y)
dim (X)
y = as.matrix(y)
dim(y)
# OLS equations
Xy=t(X)%*%y
XtX=t(X)%*%X
# Estimates, compare with fm$coef
bHat2=solve(XtX,Xy)
bHat2
# To get SEs we need an estimate of the error variance
fitV = X%*%bHat2  ## Fitted values from ourmodel
eHat=y-fitV     ### Residual
plot(eHat)        ## Sequence plot of residual
#plot(eHat ~ WAGES$Education) ### ehat vs x
plot(eHat ~ fitV)   ### Residual vs fitted value plot
vEHat=sum(eHat^2)/(nrow(gavote)-ncol(X)) # Sum of squares of errors divide by n-rank(X)
SE=sqrt(diag(solve(XtX)*vEHat)) ## SE of bHat
t_stat=bHat2/SE
## P-values (under normality first, and under t then...)
pvaluesN=pnorm(abs(t_stat),lower.tail=F)*2
pvaluesT=pt(abs(t_stat),df=length(y)-ncol(X),lower.tail=F)*2
# Regression Outputs
summary(stepwise_lm_2)
cbind(bHat2,SE,t_stat,pvaluesT)
########  OLS fit (Frequentist Inference)
summary(stepwise_lm_2)
## Coefficients of Prediction equations
bHat= coef(stepwise_lm_2)
bHat
### Gibbs sampler for a Muliple Linear Regression Model (MLR) ###
####### Bayesian Inference ###########
##  y (nx1) response  ###
### X (nxp) the covariate matrix for effects  ###
### nIter, the number of samples to be collected, set by default to 10,000.
##  Hyper-parameters: df0 and S0 (the scale and degree of freedom for the scaled-inverse chi-square prior)
##                  : b0 and varB (the mean and variance of the normal prior assigned to effects).
gibbsMLR=function(y,X,nIter=10000,df0=4,S0=var(y)*0.8*(df0-2),b0=0,varB=1e12,verbose=500){
## Objects to store samples
p=ncol(X); n=nrow(X)
B=matrix(nrow=nIter,ncol=p,0) # create a matrix to store the gibbs sample for beta
varE=rep(NA,nIter)      # .. for error variance
## Initialize
B[1,]=0     # initial values for slopes
B[1,1]=mean(y)  # initial value for y-intercept
b=B[1,]
varE[1]=var(y)  # initial error variance
resid=y-B[1,1]  # centered y (orthogonal)
## Computing sum x'x for each column
SSx=colSums(X^2)
for(i in 2:nIter){
# Sampling regression coefficients
for(j in 1:p){
A=SSx[j]/varE[i-1]+1/varB
Xy= sum(X[,j]*resid)+SSx[j]*b[j]  # equivalent to X[,j]'(y-X[,-j]%*%b[-j])
rhs=Xy/varE[i-1]  + b0/varB  # Numerator of beta^tilda_k
condMean=rhs/A
condVar=1/A
b_old=b[j]
b[j]=rnorm(n=1,mean=condMean,sd=sqrt(condVar))
B[i,j]=b[j]
resid=resid-X[,j]*(b[j]-b_old) # updating residuals
}
# Sampling the error variance
RSS=sum(resid^2)
DF=n+df0
S=RSS+S0
varE[i]=S/rchisq(df=DF,n=1)
## if(i%%verbose==0){ cat(' Iter ', i, '\n') }
}
out=list(effects=B,varE=varE)
return(out)
}
bHat
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
