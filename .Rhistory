hist(College$P.Undergrad, breaks= 45 )
#(c) v.
#Histograms for College$Room.Board
#Divide window into  2x2 matrix
par(mfrow=c(2,2))
#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$Room.Board, breaks= 2 )
hist(College$Room.Board, breaks= 6 )
hist(College$Room.Board, breaks= 9 )
hist(College$Room.Board, breaks= 45 )
#(c) v.
#Histograms for College$Books
#Divide window into  2x2 matrix
par(mfrow=c(2,2))
#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$Books, breaks= 2 )
hist(College$Books, breaks= 6 )
hist(College$Books, breaks= 9 )
hist(College$Books, breaks= 45 )
#(c) vi.
#This did not end up being useful as there are too many variables
pairs(College)
College_Private = College[College$Private == 'Yes',]
College_Public = College[College$Private == 'No',]
#Create histograms to compare college cost
par(mfrow=c(2,2))
hist(College_Private$Books)
hist(College_Public$Books)
hist(College_Private$Room.Board)
hist(College_Public$Room.Board)
#(c) vi.
avg_g_rate_priv <- mean(College_Private$Grad.Rate)
avg_g_rate_pub <- mean(College_Public$Grad.Rate)
t.test(College_Private$Grad.Rate,College_Public$Grad.Rate)
#9
#View data and remove columns with missing data points
Auto
#Removes any rows with a missing data point
Auto_rm <- na.omit(Auto)
Auto_rm
#View all variables using summary function
summary(Auto_rm)
#Create a vector of all the quantitative variables
quant_vars <- c("mpg","cylinders","displacement","horsepower","weight","acceleration")
#Create a vector of all the qualitative variables
qual_vars <- c("name","year","origin",NA,NA,NA)
#Create and display columns containing quant_vars and qual_vars (quantitative variables and qualitative variables respectively)
Variable_Type <- data.frame(quant_vars,qual_vars)
Variable_Type
#Calculates range of all quantitative variables variables
range(Auto_rm$mpg)
range(Auto_rm$cylinders)
range(Auto_rm$displacement)
range(Auto_rm$horsepower)
range(Auto_rm$weight)
range(Auto_rm$acceleration)
#(c)
#Calc mean and sd of mpg
mean(Auto_rm$mpg)
sd(Auto_rm$mpg)
#Calc mean and sd of cylinders
mean(Auto_rm$cylinders)
sd(Auto_rm$cylinders)
#Calc mean and sd of displacement
mean(Auto_rm$displacement)
sd(Auto_rm$displacement)
#Calc mean and sd of horsepower
mean(Auto_rm$horsepower)
sd(Auto_rm$horsepower)
#Calc mean and sd of weight
mean(Auto_rm$weight)
sd(Auto_rm$weight)
#Calc mean and sd of acceleration
mean(Auto_rm$acceleration)
sd(Auto_rm$acceleration)
#(d)
#Create  data set of rows 10 - 85 of 'Auto_rm'
Auto_rm2 <- Auto_rm[c(-10:-85),]
#Display new data set 'Auto_rm2'
Auto_rm2
#Returns means of key quantitative columns
colMeans(Auto_rm2[1:6])
sapply(Auto_rm2[1:6], sd)
sapply(Auto_rm2[1:6], range)
pairs(Auto_rm[1:6])
plot(lm(mpg~ ., data = Auto_rm[1:6]))
plot(lm(cylinders~ ., data = Auto_rm[1:6]))
plot(lm(displacement~ ., data = Auto_rm[1:6]))
plot(lm(horsepower~ ., data = Auto_rm[1:6]))
plot(lm(weight~ ., data = Auto_rm[1:6]))
plot(lm(acceleration~ ., data = Auto_rm[1:6]))
cor(Auto_rm[1:6])
cor(Auto_rm[1:6])>.75
#I created a heat map but it did not end up being useful
#heatmap(as.matrix(Auto_rm[1:6]), scale="column", col = cm.colors(256))
lm(mpg~.,data=Auto_rm[1:6])
summary(lm(mpg~.,data=Auto_rm[1:6]))
plot(lm(mpg~.,data=Auto_rm[1:6]))
IQ=110
GPA=4
GENDER=1
Yhat_salary = (20*GPA) + (.07*IQ)+(35*GENDER)+(.01*GPA*IQ)+(-10*(GPA*GENDER))+50
print(Yhat_salary)
resid_matrix = matrix(rep(0,5000), nrow=1000)
for(i in 1:1000){
n = 100
x = rnorm(n)
y = 5 + 2 * x + rnorm(n, 0.5)
for(j in 1:5){
resid_matrix[i,j] = sum(residuals(lm(y ~ poly(x,j,raw=T)))^2)
}
}
boxplot(resid_matrix)
#(a)
#Summarizes
summary(Auto_rm)
#Creates matrix of scatter plots containing all variables in data set
pairs(Auto_rm)
#(b)
#create matrix of correlations, excluding last name columns
pairs(cor(Auto_rm[1:8]))
model.lm1 <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm1)
plot(model.lm1)
model.lm2 <- lm(mpg~cylinders*displacement*horsepower*weight*acceleration*year*origin, data = Auto_rm)
summary(model.lm2)
#Model taking into account my intuition and prior knowledge of the data set
model.lm2 <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin+ year:mpg+year:horsepower, data = Auto_rm)
#Model taking into account my intuition and prior knowledge of the data set
model.lm3 <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin+ year:mpg+year:horsepower, data = Auto_rm)
summary(model.lm3)
model.lm2 <- lm(mpg~cylinders*displacement*horsepower*weight*acceleration*year*origin, data = Auto_rm)
summary(model.lm2)
model.lm3 <- lm(log(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm3)
model.lm4 <- lm(sqrt(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm4)
model.lm5 <- lm(mpg~cylinders+displacement+horsepower+weight+(acceleration*year)^2+origin, data = Auto_rm)
summary(model.lm5)
Carseats
Carseats_Model1 <- lm(Sales~Price+Urban+US, data = Carseats)
summary(Carseats_Model1)
Carseats
summary(Carseats_Model1)
Carseats_Model2 <- lm(Sales~Price+US, data = Carseats)
summary(Carseats_Model2)
confint(Carseats_Model2)
plot(Carseats_Model2)
set.seed(1)
x=rnorm (100)
y=2*x+rnorm (100)
lm(y~x)
lm(y~x - 1)
lm(y~x +0)
summary(lm(y~x +0))
summary(lm(y~x +0))
summary(lm(y~x))
t_new <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
t_new
summary(lm(y~x))
print(summary(lm(y~x)))
print(summary(lm(y~x)))
print(summary(lm(x~y)))
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
data("Weekly")
head(Weekly)
data("Weekly")
head(Weekly)
n <- nrow(Weekly) # Number observations in data set (1089 rows)
n_vec <- seq(1,n, by=1) # Fixed vector containing 1 - 1089
model_calc <- function(Weekly){
#Initialize empty integer (0), everytime prediction is correct add 1 to this
#Empty integer (0) for model i
i_list <- 0
#Empty integer (0) for model i
ii_list <- 0
#for loop from 1 -> n where n is the number of observations in the data set (1089 rows)
for(i in n_vec){
Weekly_working_df <- Weekly[-i,] # Create subset of data removing ith row
#Using new df with i row removed create model i and ii
logit_model_i_weekly_sub <- glm(Direction~Lag1+Lag2, data = Weekly_working_df, family = "binomial")
logit_model_ii_weekly_sub <- glm(Direction~Lag1+Lag2+I(Lag1^2)+I(Lag2^2), data = Weekly_working_df, family = "binomial")
#Posterior Probability Calculation
posterior_prob_i <-  predict.glm(logit_model_i_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
posterior_prob_ii <- predict.glm(logit_model_ii_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
#Get  posterior probability as an integer to use in boolean logic
numeric_i <- as.numeric(posterior_prob_i[1])
numeric_ii <- as.numeric(posterior_prob_ii[1])
#Set prediction to up or down dependant on posterior probability
if(numeric_i>.5){
prediction_i <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(numeric_i<=.5){
prediction_i <- "Down"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)>.5){
prediction_ii <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)<=.5){
prediction_ii <- "Down"
}
#For both models (i & ii) compare to true value and keep a count
true_value_i = Weekly[i,]$Direction
if(prediction_i == true_value_i){
i_list=i_list+1
}
if(prediction_ii == true_value_i){
ii_list=ii_list+1
}
}
print(paste("Count of true for model i is:",i_list))
print(paste("Count of true for model ii is:",ii_list))
#return(i_list)
#return(ii_list)
}
model_calc(Weekly)
i_correct <- 599 #Set given output above, count of times model i was correct
ii_correct <- 590 #Set given output above, count of times model ii was correct
#Compute test errors 1 - (Number of times model correct / row count (n) )
test_error_i <- 1-(i_correct/n)
test_error_ii <- 1-(ii_correct/n)
#Display with easily interpretable notes:
print(paste("The test error for model i is:",test_error_i))
print(paste("The test error for model ii is:",test_error_ii))
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(Weekly)), breaks=10, labels=FALSE)
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
data("College")
set.seed(20)
train <- sample(nrow(College), 600)
College.train <- College[train, ]
College.test <- College[-train, ]
head(College)
college_model1 <- lm(Apps~.,data = College.train)
MSE(College.train$Apps,as.numeric(predict(college_model1)))
X <- model.matrix(~.,data=College.train)
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
X
Y
lasso.mod <- glmnet(X[,-1], Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
dim(X)
dim(Y)
dim(X)
dim(Y)
X <- model.matrix(~.,data=College.train)[,-1]
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = bestlam)
coef(ridge.mod, s = bestlam)
dim(X)
dim(Y)
dimS
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
dim(X)
dim(Test.X)
Test.X
head(X)
head(Test.X)
head(X)
Test.X <- model.matrix(Apps~.,data=College.train)
head(Test.X)
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
head(Test.X)
X <- model.matrix(Apps~.,data=College.train)[,-1]
Test.X <- model.matrix(Apps~.,data=College.train)[,-1]
#rownames(X) <- c()
Y <- as.matrix(College.train$Apps)
cv.out <- cv.glmnet(X,Y,alpha=0,nfolds=10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(X, Y, alpha = 0, lambda = bestlam)
coef(ridge.mod, s = bestlam)
dim(X)
dim(Test.X)
head(X)
head(Test.X)
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
lasso.pred
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = bestlam)
coef(lasso.mod, s = bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = Test.X)
#lasso.pred
library(swirl)
swirl()
colors()
sample(colors(),10)
skip()
skip()
3
3
2
21
2
3
3
21
2
3
3
pal(1)
3
2
21
pal(seq(0,1,len=6))
skip()
p1(2)
p1(6)
skip()
skip()
skip()
p2(10)
p1(20)
showMe(p1(20))
showMe(p2(20))
showMe(p2(2))
p1
?fun
?rgb
1
p3 <- colorRampPalette(c("blue","green"),alpha=.5)
p3
p3(5)
skip()
skip()
skip()
showMe
showMe(cols)
colorRampPalette(cols)
pal <- colorRampPalette(cols)
showMe(pal)
showMe(pal(20))
showMe(pal(20))
image(volcano, col = pal(20))
image(volcano, col = pal(20))
image(volcano, col = p1(20))
str(mpg)
skip()
str(mpg)
qplot(displ, hwy, data = mpg, color = drv)
c()
qplot(displ, hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv)
myhigh
skip()
skip()
skip()
skip()
concat(x)
cacheGenericsMetaData(x)`
(4)
e
sss
for(x in 3){print(x)}
qplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)
skip()
skip()
summary(g)
g+geom_point()
g+geom_point()+geom_smooth()
g+geom_point()+geom_smooth(lm)
g+geom_point()+geom_smooth(string=lm)
g+geom_point()+geom_smooth(method="lm")
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
geom_line)_+ylim(-3,3)
geom_line())_+ylim(-3,3)
geom_line()_+ylim(-3,3)
geom_line()+ylim(-3,3)
g+geom_line()+ylim(-3,3)
g+geom_line()+ylim(c(-3,3)
)
g + geom_line() + coord_cartesian(ylim=c(-3,3))
mpg
g <- ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g +geom_point()
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color=black)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color="black")
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(size=2,method=lm,se=FALSE,color="black")
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black"
)
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black")+labs(x="Displacement",y="Highway Mileage",title="Swirl Rules!")
str(diamonds)
2
22
2
qplot(price,data=diamonds)
range(diamonds$price)
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds,color=cut)
plot(data=carat,price,data=diamonds,color=cut)
plot(data=diamonds,color=cut)
skip()
skip()
2
2
skip()
summary(g)
g+geom_point(alpha=1/3)
summary(g)
cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)
cutpoints
skip()
skip()
skip()
diamonds[myd,]
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smooth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
2
2
knitr::opts_chunk$set(echo = TRUE)
wages
Wages
data(Wages)
library(ISLR)
data(Wages)
data("Wage")
Wage
rm(list=ls())
setwd("C:/Users/sam/Desktop/STT465Bayesian")
df = read.csv("wages")
df = read.table("wages.txt")
head(df)
df = read.table("wages.txt",header = TRUE)
dim(df)
head(df)
