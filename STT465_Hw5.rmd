---
title: "STT465_HW5"
author: "Sam Isken"
date: "November 8, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

An Example: MLR Analysis in Frequentist and Bayesian

We will disscuss most of this today and you should compile the work and submit as Homework #5 (Due Monday 11/11/2019)

1. Load the WAGES dataset into R: 

```{r}
WAGES <- read.table("wages.txt",header = TRUE)
head(WAGES)
```

2. Conduct basic descriptive statistical analysis: 

Make comments on descriptive statistics and constrict a scatterplot matrix: 

```{r}
pairs(WAGES)
summary(WAGES)
```

3.Regress wage on education, race, experience, region, sex and marital status via OLS using lm():

```{r}
lin_model_1 <- lm(Wage~Education+Black+Hispanic+Experience+South+Sex+Married,data=WAGES)
```

4.Using the output of summary(lm(y~....)) answer the following questions: 

```{r}
summary(lin_model_1)
```


a) What of the factors / variables have a significant effect on wages? 

Education, Experience, Region (located in South or not, binary variable 1 or 0) and Sex are the 4 variables (besides the intercept) that are statistically significant in this model. 

b) How much do you expect wage to increase per year of education? 

Given a 1 year increase in education  we estimate wages to increase by .93.

c) What is the average wage difference between male and female in the sample? 

2.33784

d) Is the difference in c) statistically different from 0?

Yes

5. Load the Gibbs Sampler Function in R: 

```{r}
### Gibbs Sampler #######

gibbsMLR=function(y,X,nIter=10000,df0=4,S0=var(y)*0.8*(df0-2),b0=0,varB=1e12,verbose=500){
  
  ## Objects to store samples
  p=ncol(X); n=nrow(X)
  B=matrix(nrow=nIter,ncol=p,0) # create a matrix to store the gibbs sample for beta
  varE=rep(NA,nIter)      # .. for error variance
  
  ## Initialize
  B[1,]=0     # initial values for slopes
  B[1,1]=mean(y)  # initial value for y-intercept
  b=B[1,]
  varE[1]=var(y)  # initial error variance
  resid=y-B[1,1]  # centered y (orthogonal)
  
  ## Computing sum x'x for each column
  SSx=colSums(X^2)
  
  for(i in 2:nIter){
    # Sampling regression coefficients
    for(j in 1:p){
      A=SSx[j]/varE[i-1]+1/varB
      Xy= sum(X[,j]*resid)+SSx[j]*b[j]  # equivalent to X[,j]'(y-X[,-j]%*%b[-j])
      rhs=Xy/varE[i-1]  + b0/varB  # Numerator of beta^tilda_k
      condMean=rhs/A
      condVar=1/A
      b_old=b[j]
      b[j]=rnorm(n=1,mean=condMean,sd=sqrt(condVar))
      B[i,j]=b[j]  
      resid=resid-X[,j]*(b[j]-b_old) # updating residuals
    }
    # Sampling the error variance  
    RSS=sum(resid^2)
    DF=n+df0
    S=RSS+S0
    varE[i]=S/rchisq(df=DF,n=1)
    
   ## if(i%%verbose==0){ cat(' Iter ', i, '\n') }
  }
  
  out=list(effects=B,varE=varE)
  return(out)
}
```

6. Collect, for the same model specified abovem 15,000 samples. 

Hint: gibbsMLR(y,X,..) takes as inputs a numeric vector with the response (WAGES) and an incidence matrix for effects (sex,race,education,experience,...). lm() creates the incidence  matrix internally. For this you can use X=model.matrix(~a+b+...,date=WAGES) where a,b are the predictors that are included in data. This will create your incidence matrix and then you will use it in gibbsMLR(y,X). 


7. Conduct post-gibbs analysis (trace plot, auto-correlation, decide on burin-in and thinning, provide posterior means, posterior SDs and posterior credibility regions, estimate and report MC errror).

8. Test whether there is an ethnic disparity between black and Hispanic workers and report the posterior probability. 